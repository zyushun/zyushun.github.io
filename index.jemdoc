# jemdoc: menu{MENU}{index.html}, nofooter
==Yushun Zhang

~~~
{}{img_left}{images/red.jpeg}{alt text}{179}{239}{}
Yushun Zhang (张雨舜)\n
Ph.D student,\n
School of Data Science, \n
The Chinese University of Hong Kong, Shenzhen, China 

Email: yushunzhang AT link DOT cuhk DOT edu DOT cn 

[https://scholar.google.com/citations?user=dHUiyDkAAAAJ&hl=en \[Google Scholar\]] 
[https://twitter.com/ericzhang0410 \[Twitter\]] [https://github.com/zyushun \[Github\]] [https://www.instagram.com/ericzhang0410/ \[Instagram\]]
~~~

== About me
I'm a Ph.D student in School of Data Science at The Chinese University of Hong Kong, Shenzhen, China. I'm very proud to be advised by [https://scholar.google.com/citations?user=dW3gcXoAAAAJ&hl=en Prof. Zhi-Quan (Tom) Luo].  I’m also very fortunate to work closely with [https://ruoyus.github.io Prof. Ruoyu Sun]. 
Previously, I did my undergraduate study in the Department of Mathematics at Southern University of Science and Technology (SUSTech). 


 My research focuses on optimization, deep learning, and especially, large language models. I aim to work on important and practical problems with optimization flavor. 

== Research that I lead or co-lead

[http://arxiv.org/abs/2406.16793 Adam-mini:  Use Fewer Learning Rates To Gain More] \n
*Yushun Zhang\* *, Congliang Chen\*,  Ziniu Li, Tian Ding, Chenwei Wu, Diederik P. Kingma, Yinyu Ye, Zhi-Quan Luo, Ruoyu Sun \n
Preprint



[https://arxiv.org/abs/2402.16788 Why Transformers Need Adam: A Hessian Perspective] \n
*Yushun Zhang*, Congliang Chen, Tian Ding, Ziniu Li, Ruoyu Sun, Zhi-Quan Luo \n
NeurIPS 2024


[https://nips.cc/virtual/2022/spotlight/65228 Adam Can Converge Without Any Modification on Update Rules]  \n
*Yushun Zhang*, Congliang Chen, Naichen Shi, Ruoyu Sun,  Zhi-Quan Luo  \n
NeurIPS 2022 


[https://iclr-blog-track.github.io/2022/03/25/does-adam/ Does Adam Converge and When?] \n
*Yushun Zhang*, Congliang Chen,  Zhi-Quan Luo \n
ICLR Blog Track 2022

[https://openreview.net/forum?id=K_MD-PMTLtA When Expressivity Meets Trainability: Fewer than n Neurons Can Work]  \n
Jiawei Zhang\*, *Yushun Zhang\* *, Mingyi Hong, Ruoyu Sun, Zhi-Quan Luo \n
NeurIPS 2021

== Research that I proudly participate in


[https://arxiv.org/abs/2310.10505 ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models] \n
Ziniu Li, Tian Xu, *Yushun Zhang*, Zhihang Lin, Yang Yu, Ruoyu Sun, Zhi-Quan Luo \n
ICML 2024

[https://arxiv.org/abs/2208.09900 Provable Adaptivity of Adam under Non-Uniform Smoothness]  \n
Bohan Wang\*, *Yushun Zhang\**, Huishuai Zhang, Qi Meng, Ruoyu Sun, Zhi-Ming Ma, Zhi-Quan Luo, Tie-Yan Liu, Wei Chen \n
KDD 2024


[https://openreview.net/forum?id=X0nrKAXu7g-  HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning]  \n
Ziniu Li, Yingru Li, *Yushun Zhang*, Tong Zhang, Zhi-Quan Luo \n
ICLR 2022 \n
(This work is also selected as  Oral presentation at [https://neurips.cc/Conferences/2021/ScheduleMultitrack?event=21865 NeurIPS workshop, 2021]) \n




[\href{https://www.tandfonline.com/doi/abs/10.1080/10543406.2020.1814794  Fast QLB algorithm and hypothesis tests in logistic model for ophthalmologic bilateral correlated data] \n
 Yiqi Lin\*, *Yushun Zhang\* *, Guoliang Tian, Changxing Ma \n
 Journal of Biopharmaceutical Statistics 2020 \n
 (This work is done in Sustech) 



== Invited Talks

*Oct 2024*: I gave a talk at U of Minnesota, hosted by [https://scholar.google.com/citations?user=qRnP-p0AAAAJ&hl=en Prof. Mingyi Hong]. Thanks Prof. Hong for the invitation!

- Topic: Why Transformers Need Adam: A Hessian Perspective
- Slides can be seen [files/1024_umntalk.pdf here (this is a >40 min version with Adam-mini included)]

*Oct 2024*: I gave a talk at INFORMS Anneal Meeting, Seattle, hosted by [https://scholar.google.com/citations?user=7ubAY0UAAAAJ&hl=en Jianhao Ma]. Thanks Jianhao for the invitation!

- Topic: Why Transformers Need Adam: A Hessian Perspective
- Slides can be seen [files/spectrum_talk_informs.pdf here (this is a 15 min version)]

*Sep 2023*: I gave a talk at Tsinghua University, hosted by [https://scholar.google.com/citations?user=zX7i1EkAAAAJ&hl=en Prof. Jian Li]. Thanks Prof. Li for the invitation!

- Topic: Converge or Diverge? A Story of Adam
- Slides can be seen [files/Adam_talk_Tsinghua.pdf here]



*Jan 2023*: I gave a talk at Google Brain, hosted by [https://scholar.google.nl/citations?user=yyIoQu4AAAAJ&hl=en Dr. Diederik P. Kingma]. Thanks Dr. Kingma for the invitation! 

- Topic: Adam Can Converge Without Any Modification on Update Rules 
- Slides can be seen  [files/Adam_talk_google_short.pdf here]


== Awards (By time)


Duan Yongping Outstanding Resesearch Award (1st place), 2023

Teaching Assistant Award, School of Data Science, 2023


Best Paper Presentation Award (1st place), [https://mp.weixin.qq.com/s/IBED0dwbegS0PZQMuk656Q 2nd Doctoral and Postdoctoral Daoyuan Academic Forum, 2022]\n
- Topic: Does Adam Converge and When?\n
- Slides can be seen  [files/Adam_talk_sribd.pptx here].\n
- A short version of this talk can be viewed [https://nips.cc/virtual/2022/poster/53721 here].




Best Paper Presentation Award (1st place), [https://www.tbsi.edu.cn/wolt/index.html 3rd Tsinghua-Berkeley workshop on Learning Theory, 2021]\n
- Topic: When Expressivity Meets Trainability: Width < n Can Work\n
- A short version of this talk can be viewed [https://slideslive.com/38970555/when-expressivity-meets-trainability-fewer-than-n-neurons-can-work?ref=recommended here].

Magna cum laude of SUSTech, 2019

Outstanding graduation thesis, SUSTech, 2019

Scholarship Award for Excellence, Mathematics department, SUSTech (Top 10 students) , 2018




== Services

==== Reviewer

I serve as a reviewer for machine learning conferences including NeurIPS, ICLR, ICML, COLT, AISTATS, as well as journals including JMLR and TMLR.

==== Social Activities

I hosted a session named [https://submissions.mirasmart.com/InformsAnnual2024/Itinerary/EventDetail.aspx?evt=956 "Optimization Issues in Recent AI Models"] at INFORMS Anneal Meeting, Oct, 2024.

==== Teaching Assistant (by time)

- DDA4300: Optimization for Machine Learning, by Prof. Yinyu Ye (2023 Spring)

- DDA 6060: Machine Learning, by Prof. Hongyuan Zha \& Prof. Shuang Li (2022 Spring)

- DDA 4002: Multivariate Statistics, by Prof. Zhaoyuan Li (2021 Autumn)

- DDA 4250: Mathematics for Deep Learning, by Prof. Arnulf Jentzen (2021 Spring)

- MFE 5100: Optimization, by Prof. Zizhuo Wang (2020 Autumn)

- STA 2002: Probalility and Statistics, by Prof. Xinyun Chen  (2020 Summer)

-  CSC 4020: Fundationals of Machine Learning, by Prof. Hongyuan Zha (2020 Spring)

- MAT 2040: Linear algebra, by Prof. Shenghao Yang (2019 Autumn)

- MAT 7035: Computational Statistics, by Prof. Guoliang Tian (SUSTech) (2018 Autumn)
 
- MA 204: Mathematical Statistics, by Prof. Guoliang Tian (SUSTech)  (2018 Spring)


== Experiences


I spent a great time as an exchange undergraduate student at Mathematics department, UC San Diego, 2019 Spring.

I spent the best three years at the Shenzhen Foreign Language School, branch, 2009 - 2012.










