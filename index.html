<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
<title>Yushun Zhang</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Yushun Zhang</h1>
</div>
<table class="imgtable"><tr><td>
<img src="images/red.jpeg" alt="alt text" width="179px" height="239px" />&nbsp;</td>
<td align="left"><p>Yushun Zhang (张雨舜)<br />
Ph.D student,<br />
School of Data Science, <br />
The Chinese University of Hong Kong, Shenzhen, China </p>
<p>Email: yushunzhang AT link DOT cuhk DOT edu DOT cn </p>
<p><a href="https://scholar.google.com/citations?user=dHUiyDkAAAAJ&amp;hl=en">[Google Scholar]</a> 
<a href="https://twitter.com/ericzhang0410">[Twitter]</a> <a href="https://github.com/zyushun">[Github]</a> <a href="https://www.instagram.com/ericzhang0410/">[Instagram]</a></p>
</td></tr></table>
<h2>About me</h2>
<p>I'm a Ph.D student in School of Data Science at The Chinese University of Hong Kong, Shenzhen, China. I'm very proud to be advised by <a href="https://scholar.google.com/citations?user=dW3gcXoAAAAJ&amp;hl=en">Prof. Zhi-Quan (Tom) Luo</a>.  I’m also very fortunate to work closely with <a href="https://ruoyus.github.io">Prof. Ruoyu Sun</a>. 
Previously, I did my undergraduate study in the Department of Mathematics at Southern University of Science and Technology (SUSTech). </p>
<p>My research focuses on optimization, deep learning, and especially, large language models. I aim to solve real and practical problems in these areas. </p>
<h2>Research that I lead or co-lead</h2>
<p><a href="http://arxiv.org/abs/2406.16793">Adam-mini:  Use Fewer Learning Rates To Gain More</a> <br />
<b>Yushun Zhang* </b>, Congliang Chen*,  Ziniu Li, Tian Ding, Chenwei Wu, Yinyu Ye, Zhi-Quan Luo, Ruoyu Sun <br />
Preprint</p>
<p><a href="https://arxiv.org/abs/2402.16788">Why Transformers Need Adam: A Hessian Perspective</a> <br />
<b>Yushun Zhang</b>, Congliang Chen, Tian Ding, Ziniu Li, Ruoyu Sun, Zhi-Quan Luo <br />
Preprint</p>
<p><a href="https://nips.cc/virtual/2022/spotlight/65228">Adam Can Converge Without Any Modification on Update Rules</a>  <br />
<b>Yushun Zhang</b>, Congliang Chen, Naichen Shi, Ruoyu Sun,  Zhi-Quan Luo  <br />
NeurIPS 2022 </p>
<p><a href="https://iclr-blog-track.github.io/2022/03/25/does-adam/">Does Adam Converge and When?</a> <br />
<b>Yushun Zhang</b>, Congliang Chen,  Zhi-Quan Luo <br />
ICLR Blog Track 2022</p>
<p><a href="https://openreview.net/forum?id=K_MD-PMTLtA">When Expressivity Meets Trainability: Fewer than n Neurons Can Work</a>  <br />
Jiawei Zhang*, <b>Yushun Zhang* </b>, Mingyi Hong, Ruoyu Sun, Zhi-Quan Luo <br />
NeurIPS 2021</p>
<h2>Research that I proudly participate in</h2>
<p><a href="https://arxiv.org/abs/2310.10505">ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models</a> <br />
Ziniu Li, Tian Xu, <b>Yushun Zhang</b>, Zhihang Lin, Yang Yu, Ruoyu Sun, Zhi-Quan Luo <br />
ICML 2024</p>
<p><a href="https://arxiv.org/abs/2208.09900">Provable Adaptivity of Adam under Non-Uniform Smoothness</a>  <br />
Bohan Wang*, <b>Yushun Zhang*</b>, Huishuai Zhang, Qi Meng, Ruoyu Sun, Zhi-Ming Ma, Zhi-Quan Luo, Tie-Yan Liu, Wei Chen <br />
KDD 2024</p>
<p><a href="https://openreview.net/forum?id=X0nrKAXu7g-">HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning</a>  <br />
Ziniu Li, Yingru Li, <b>Yushun Zhang</b>, Tong Zhang, Zhi-Quan Luo <br />
ICLR 2022 <br />
(This work is also selected as  Oral presentation at <a href="https://neurips.cc/Conferences/2021/ScheduleMultitrack?event=21865">NeurIPS workshop, 2021</a>) <br /></p>
<p><a href="\href{https://www.tandfonline.com/doi/abs/10.1080/10543406.2020.1814794">Fast QLB algorithm and hypothesis tests in logistic model for ophthalmologic bilateral correlated data</a> <br />
Yiqi Lin*, <b>Yushun Zhang* </b>, Guoliang Tian, Changxing Ma <br />
Journal of Biopharmaceutical Statistics 2020 <br />
(This work is done in Sustech) </p>
<h2>Invited Talks</h2>
<p><b>Sep 2023</b>: I gave a talk at Tsinghua University, hosted by <a href="https://scholar.google.com/citations?user=zX7i1EkAAAAJ&amp;hl=en">Prof. Jian Li</a>. Thanks Prof. Li for the invitation!</p>
<ul>
<li><p>Topic: Converge or Diverge? A Story of Adam</p>
</li>
<li><p>Slides can be seen <a href="files/Adam_talk_Tsinghua.pdf">here</a></p>
</li>
</ul>
<p><b>Jan 2023</b>: I gave a talk at Google Brain, hosted by <a href="https://scholar.google.nl/citations?user=yyIoQu4AAAAJ&amp;hl=en">Dr. Diederik P. Kingma</a>. Thanks Dr. Kingma for the invitation! </p>
<ul>
<li><p>Topic: Adam Can Converge Without Any Modification on Update Rules </p>
</li>
<li><p>Slides can be seen  <a href="files/Adam_talk_google_short.pdf">here</a></p>
</li>
</ul>
<h2>Awards (By time)</h2>
<p>Duan Yongping Outstanding Resesearch Award (1st place), 2023</p>
<p>Teaching Assistant Award, School of Data Science, 2023</p>
<p>Best Paper Presentation Award (1st place), <a href="https://mp.weixin.qq.com/s/IBED0dwbegS0PZQMuk656Q">2nd Doctoral and Postdoctoral Daoyuan Academic Forum, 2022</a><br /></p>
<ul>
<li><p>Topic: Does Adam Converge and When?<br /></p>
</li>
<li><p>Slides can be seen  <a href="files/Adam_talk_sribd.pptx">here</a>.<br /></p>
</li>
<li><p>A short version of this talk can be viewed <a href="https://nips.cc/virtual/2022/poster/53721">here</a>.</p>
</li>
</ul>
<p>Best Paper Presentation Award (1st place), <a href="https://www.tbsi.edu.cn/wolt/index.html">3rd Tsinghua-Berkeley workshop on Learning Theory, 2021</a><br /></p>
<ul>
<li><p>Topic: When Expressivity Meets Trainability: Width &lt; n Can Work<br /></p>
</li>
<li><p>A short version of this talk can be viewed <a href="https://slideslive.com/38970555/when-expressivity-meets-trainability-fewer-than-n-neurons-can-work?ref=recommended">here</a>.</p>
</li>
</ul>
<p>Magna cum laude of SUSTech, 2019</p>
<p>Outstanding graduation thesis, SUSTech, 2019</p>
<p>Scholarship Award for Excellence, Mathematics department, SUSTech (Top 10 students) , 2018</p>
<h2>Services</h2>
<h4>Reviewer</h4>
<p>I serve as a reviewer for machine learning conferences including NeurIPS, ICLR, ICML, COLT, AISTATS, as well as journals including JMLR and TMLR.</p>
<h4>Teaching Assistant (by time)</h4>
<ul>
<li><p>DDA4300: Optimization for Machine Learning, by Prof. Yinyu Ye (2023 Spring)</p>
</li>
</ul>
<ul>
<li><p>DDA 6060: Machine Learning, by Prof. Hongyuan Zha & Prof. Shuang Li (2022 Spring)</p>
</li>
</ul>
<ul>
<li><p>DDA 4002: Multivariate Statistics, by Prof. Zhaoyuan Li (2021 Autumn)</p>
</li>
</ul>
<ul>
<li><p>DDA 4250: Mathematics for Deep Learning, by Prof. Arnulf Jentzen (2021 Spring)</p>
</li>
</ul>
<ul>
<li><p>MFE 5100: Optimization, by Prof. Zizhuo Wang (2020 Autumn)</p>
</li>
</ul>
<ul>
<li><p>STA 2002: Probalility and Statistics, by Prof. Xinyun Chen  (2020 Summer)</p>
</li>
</ul>
<ul>
<li><p>CSC 4020: Fundationals of Machine Learning, by Prof. Hongyuan Zha (2020 Spring)</p>
</li>
</ul>
<ul>
<li><p>MAT 2040: Linear algebra, by Prof. Shenghao Yang (2019 Autumn)</p>
</li>
</ul>
<ul>
<li><p>MAT 7035: Computational Statistics, by Prof. Guoliang Tian (SUSTech) (2018 Autumn)</p>
</li>
</ul>
<ul>
<li><p>MA 204: Mathematical Statistics, by Prof. Guoliang Tian (SUSTech)  (2018 Spring)</p>
</li>
</ul>
<h2>Experiences</h2>
<p>I spent a great time as an exchange undergraduate student at Mathematics department, UC San Diego, 2019 Spring.</p>
<p>I spent the best three years at the Shenzhen Foreign Language School, branch, 2009 - 2012.</p>
</td>
</tr>
</table>
</body>
</html>
